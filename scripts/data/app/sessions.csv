Session ID,Session Title,Date,Time Start,Time End,Room/Location,Schedule Track,Description
1,Morning Tutorials,10/31/18,9:00 AM,12:30 PM,,Conference Sessions,
2,"[T1] Dive into Deep Learning for Natural Language Processing",11/03/19,9:00 AM,12:30 PM,Gold Hall,Tutorials,"Deep learning has become the dominant approach to NLP problems, especially when applied on large scale corpora. Recent progress on unsupervised pre-training techniques such as BERT, ELMo, GPT-2, and language modeling in general, when applied on large corpora, is shown to be effective in improving a wide variety of downstream tasks. These techniques push the limits of available hardware, requiring specialized frameworks optimized for GPU, ASIC, and distributed cloud-based training.

A few complexities pose challenges to scale these models and algorithms effectively. Compared to other areas where deep learning is applied, these NLP models contain a variety of moving parts: text normalization and tokenization, word representation at subword-level and word-level, variable-length models such as RNN and attention, and sequential decoder based on beam search, among others.

In this hands-on tutorial, we take a closer look at the challenges from these complexities and see how with proper tooling with Apache MXNet and GluonNLP, we can overcome these challenges and achieve state-of-the-art results for real-world problems. GluonNLP is a powerful new toolkit that combines MXNetâ€™s speed, the flexibility of Gluon, and an extensive new library automating the most laborious aspects of deep learning for NLP."
3,"[T2] Processing and Understanding Mixed Language Data",11/03/19,9:00 AM,12:30 PM,Hall 100,Tutorials,"Multilingual communities exhibit code-mixing, that is, mixing of two or more socially stable languages in a single conversation, sometimes even in a single utterance. This phenomenon has been widely studied by linguists and interaction scientists in the spoken language of such communities. However, with the prevalence of social media and other informal interactive platforms, code-switching is now also ubiquitously observed in user-generated text. As multilingual communities are more the norm from a global perspective, it becomes essential that code-switched text and speech are adequately handled by language technologies and NUIs.

Code-mixing is extremely prevalent in all multilingual societies. Current studies have shown that as much as 20% of user generated content from some geographies, like South Asia, parts of Europe, and Singapore, are code-mixed. Thus, it is very important to handle code-mixed content as a part of NLP systems and applications for these geographies.

In the past 5 years, there has been an active interest in computational models for code-mixing with a substantive research outcome in terms of publications, datasets and systems. However, it is not easy to find a single point of access for a complete and coherent overview of the research. This tutorial is expecting to fill this gap and provide new researchers in the area with a foundation in both linguistic and computational aspects of code-mixing. We hope that this then becomes a starting point for those who wish to pursue research, design, development and deployment of code-mixed systems in multilingual societies."
4,Lunch,11/03/19,12:30 PM,2:00 PM,,Conference Sessions,
5,Afternoon Tutorials,11/03/19,2:00 PM,5:30 PM,,Conference Sessions,
6,"[T1] Dive into Deep Learning for Natural Language Processing (cont.)",11/03/19,14:00,17:30,Gold Hall,Tutorials,"Continuation of morning tutorial."
7,"[T3] Data Collection and End-to-End Learning for Conversational AI",11/03/19,14:00,17:30,Gold Hall,Tutorials,"A fundamental long-term goal of conversational AI is to merge two main dialogue system paradigms into a standalone multi-purpose system. Such a system should be capable of conversing about arbitrary topics (Paradigm 1: open-domain dialogue systems), and simultaneously assist humans with completing a wide range of tasks with well-defined semantics such as restaurant search and booking, customer service applications, or ticket bookings (Paradigm 2: task-based dialogue systems).

The recent developmental leaps in conversational AI technology are undoubtedly linked to more and more sophisticated deep learning algorithms that capture patterns in increasing amounts of data generated by various data collection mechanisms. The goal of this tutorial is therefore twofold. First, it aims at familiarising the research community with the recent advances in algorithmic design of statistical dialogue systems for both open-domain and task-based dialogue paradigms. The focus of the tutorial is on recently introduced end-to-end learning for dialogue systems and their relation to more common modular systems. In theory, learning end-to-end from data offers seamless and unprecedented portability of dialogue systems to a wide spectrum of tasks and languages. From a practical point of view, there are still plenty of research challenges and opportunities remaining: in this tutorial we analyse this gap between theory and practice, and introduce the research community with the main advantages as well as with key practical limitations of current end-to-end dialogue learning.

The critical requirement of each statistical dialogue system is the data at hand. The system cannot provide assistance for the task without having appropriate task-related data to learn from. Therefore, the second major goal of this tutorial is to provide a comprehensive overview of the current approaches to data collection for dialogue, and analyse the current gaps and challenges with diverse data collection protocols, as well as their relation to and current limitations of data-driven end-to-end dialogue modeling. We will again analyse this relation and limitations both from research and industry perspective, and provide key insights on the application of state-of-the-art methodology into industry-scale conversational AI systems."
8,"[W1] CoNLL: The Conference on Computational Natural Language Learning (Day 1)",11/03/19,8:30 AM,6:00 PM,Copper Hall / Studio 311-312,Workshops,"The Conference on Computational Natural Language Learning is a top-tier conference, yearly organized by SIGNLL (ACLâ€™s Special Interest Group on Natural Language Learning). <a href='http://www.conll.org/'>[Link]</a>"
9,"[W2] FEVER",11/03/19,8:30 AM,6:00 PM,TBD,Workshops,"The second workshop on Fact Extraction and VERification brings together researchers working on various tasks related to fact extraction and verification and also hosts the FEVER Challenge, an information verification shared task. <a href='http://fever.ai'>[Link]</a>"
10,"[W3] DiscoMT19",11/03/19,8:30 AM,6:00 PM,TBD,Workshops,"Discourse in Machine Translation 2019 focuses on language processing techniques, whether theoretically-inspired or empirical, which address one or more discourse-level phenomena in combination with MT or from a cross-lingual perspective. <a href='https://www.idiap.ch/workshop/DiscoMT'>[Link]</a>"
11,"[W4] LANTERN",11/03/19,8:30 AM,6:00 PM,TBD,Workshops,"Beyond Vision and Language: Integrating Knowledge from the Real World aims to to bring together researchers from different disciplines but united by their adoption of techniques from machine learning, neuroscience, multi-agents, natural language processing, computer vision and psychology to interconnect language and vision by leveraging external knowledge provided by either fixed or environments. <a href='https://www.lantern.uni-saarland.de/'>[Link]</a>"
12,"[W5] MSR 2019",11/03/19,8:30 AM,6:00 PM,TBD,Workshops,"The second Workshop on Multilingual Surface Realization aims at bringing together people who are interested in surface-oriented Natural Language Generation problems such as word order determination, inflection, functional word determination, paraphrasing, etc. <a href='http://taln.upf.edu/pages/msr2019-ws/'>[Link]</a>"
13,"[W6] LOUHI 2019",11/03/19,8:30 AM,6:00 PM,TBD,Workshops,"The Tenth International Workshop on Health Text Mining and Information Analysis provides an interdisciplinary forum for researchers interested in automated processing of health documents. <a href='http://louhi2019.fbk.eu/'>[Link]</a>"
14,"[W7] DeepLo 2019",11/03/19,8:30 AM,6:00 PM,TBD,Workshops,"Deep Learning for Low-Resource Natural Language Processing is a workshop that will bring together experts in deep learning and natural language processing whose research focuses on learning with scarce data. <a href='https://sites.google.com/view/deeplo19/'>[Link]</a>"
15,"[W8] COIN",11/03/19,8:30 AM,6:00 PM,TBD,Workshops,"COmmonsense INference in NLP is a workshop that aims at bringing together researchers that are interested in modeling commonsense knowledge, developing computational models thereof, and applying commonsense inference methods in NLP tasks. <a href='http://www.coli.uni-saarland.de/~mroth/COIN/'>[Link]</a>"
16,"[W9] AnnoNLP",11/03/19,8:30 AM,6:00 PM,TBD,Workshops,"Aggregating and analysing crowdsourced annotations for NLP Although there is a large body of work analysing crowdsourced data, be that probabilistic or traditional, there has been less work devoted to NLP tasks. The aim of the proposed workshop is to bring together the community of researchers interested in this area. <a href='http://dali.eecs.qmul.ac.uk/annonlp'>[Link]</a>"
17,Morning Tutorials,11/04/18,9:00 AM,12:30 PM,,Conference Sessions,
18,"[T4] Bias and Fairness in Natural Language Processing",11/04/19,9:00 AM,12:30 PM,Gold Hall,Tutorials,"Recent advances in data-driven machine learning techniques (e.g., deep neural networks) have revolutionized many natural language processing applications. These approaches automatically learn how to make decisions based on the statistics and diagnostic information from large amounts of training data. Despite the remarkable accuracy of machine learning in various applications, learning algorithms run the risk of relying on societal biases encoded in the training data to make predictions. This often occurs even when gender and ethnicity information is not explicitly provided to the system because learning algorithms are able to discover implicit associations between individuals and their demographic information based on other variables such as names, titles, home addresses, etc. Therefore, machine learning algorithms risk potentially encouraging unfair and discriminatory decision making and raise serious privacy concerns. Without properly quantifying and reducing the reliance on such correlations, broad adoption of these models might have the undesirable effect of magnifying harmful stereotypes or implicit biases that rely on sensitive demographic attributes.

In this tutorial, we will review the history of bias and fairness studies in machine learning and language processing and present recent community effort in quantifying and mitigating bias in natural language processing models for a wide spectrum of tasks, including word embeddings, co-reference resolution, machine translation, and vision-and-language tasks. In particular, we will focus on the following topics: (a) Definitions of fairness and bias. (b) Data, algorithms, and models that propagate and even amplify social bias to NLP applications and metrics to quantify these biases. (c) Algorithmic solutions; learning objective; design principles to prevent social bias in NLP systems and their potential drawbacks.

The tutorial will bring researchers and practitioners to be aware of this issue, and encourage the research community to propose innovative solutions to promote fairness in NLP."
19,"[T5] Discreteness in Neural Natural Language Processing",11/04/19,9:00 AM,12:30 PM,Gold Hall,Tutorials,"This tutorial provides a comprehensive guide to the process of discreteness in neural NLP.

As a gentle start, we will briefly introduce the background of deep learning based NLP, where we point out the ubiquitous discreteness of natural language and its challenges in neural information processing. Particularly, we will focus on how such discreteness plays a role in the input space, the latent space, and the output space of a neural network. In each part, we will provide examples, discuss machine learning techniques, as well as demonstrate NLP applications."
20,Lunch,11/04/19,12:30 PM,2:00 PM,,Conference Sessions,
21,Afternoon Tutorials,11/04/19,2:00 PM,5:30 PM,,Conference Sessions,
22,"[T6] Graph-based Deep Learning in Natural Language Processing",11/04/19,14:00,17:30,Gold Hall,Tutorials,"This tutorial aims to introduce recent advances in graph-based deep learning techniques such as Graph Convolutional Networks (GCNs) for Natural Language Processing (NLP). It provides a brief introduction to deep learning methods on non-Euclidean domains such as graphs and justifies their relevance in NLP. It then covers recent advances in applying graph-based deep learning methods for various NLP tasks, such as semantic role labeling, machine translation, relationship extraction, and many more."
23,"[T7] Semantic Specialization of Distributional Word Vectors",11/04/19,14:00,17:30,Gold Hall,Tutorials,"Distributional word vectors have become an indispensable component of most state-of-art NLP models. As a major artefact of the underlying distributional hypothesis, distributional word vector spaces conflate various paradigmatic and syntagmatic lexico-semantic relations. For example, relations such as synonymy/similarity (e.g., car-automobile) or lexical entailment (e.g., car-vehicle) often cannot be distinguished from antonymy (e.g., black-white), meronymy (e.g., car-wheel) or broader thematic relatedness (e.g., car-driver) based on the distances in the distributional vector space. This inherent property of distributional spaces often harms performance in downstream applications, since different lexico-semantic relations support different classes of NLP applications. For instance, Semantic Similarity provides guidance for Paraphrasing, Dialogue State Tracking, and Text Simplification, Lexical Entailment supports Natural Language Inference and Taxonomy Induction, whereas broader thematic relatedness yields gains for Named Entity Recognition, Parsing, and Text Classification and Retrieval.

A plethora of methods have been proposed to emphasize specific lexico-semantic relations in a reshaped (i.e., specialized) vector space. A common solution is to move beyond purely unsupervised word representation learning and include external lexico-semantic knowledge, in a process commonly referred to as semantic specialization. In this tutorial, we provide a thorough overview of specialization methods, covering: 1) joint specialization methods, which augment distributional learning objectives with external linguistic constraints, 2) post-processing retrofitting models, which fine-tune pre-trained distributional vectors to better reflect external linguistic constraints, and 3) the most recently proposed post-specialization methods that generalize the perturbations of the post-processing methods to the whole distributional space. In addition to providing a comprehensive overview of specialization methods, we will introduce the most recent developments, such as (among others): handling asymmetric relations (e.g., hypernymy-hyponymy) in Euclidean and hyperbolic spaces by accounting for vector magnitude as well as for vector distance; cross-lingual transfer of semantic specialization for languages without external lexico-semantic resources; downstream effects of specializing distributional vector spaces; injecting external knowledge into unsupervised pretraining architectures such as ELMo or BERT."
24,"[W1] CoNLL: The Conference on Computational Natural Language Learning (Day 2)",11/04/19,8:30 AM,6:00 PM,Copper Hall / Studio 311-312,Workshops,"The Conference on Computational Natural Language Learning is a top-tier conference, yearly organized by SIGNLL (ACLâ€™s Special Interest Group on Natural Language Learning). <a href='http://www.conll.org'>[Link]</a>"
25,"[W10] MRQA",11/04/19,8:30 AM,6:00 PM,TBD,Workshops,"The 2nd Workshop on Machine Reading for Question Answering focuses on understanding and improving methods for question answering (QA) from the text. This year, the workshop will host a shared task on QA with an emphasis on generalization. <a href='https://mrqa.github.io/'>[Link]</a>"
26,"[W11] BioNLP-OST 2019",11/04/19,8:30 AM,6:00 PM,TBD,Workshops,"International Workshop on BioNLP Open Shared Tasks 2019 is organized to facilitate development and sharing of computational tasks of biomedical text mining and solutions to them. <a href='http://2019.bionlp-ost.org/'>[Link]</a>"
27,"[W12] WNGT",11/04/19,8:30 AM,6:00 PM,TBD,Workshops,"The 3rd Workshop on Neural Generation and Translation aims to provide a forum for research in applications of neural models to language generation and translation tasks. <a href='https://sites.google.com/view/wngt19/home'>[Link]</a>"
28,"[W13] W-NUT 2019",11/04/19,8:30 AM,6:00 PM,TBD,Workshops,"The 5th Workshop on Noisy User-generated Text focuses on Natural Language Processing applied to noisy user-generated text, such as that found in social media, online reviews, crowdsourced data, web forums, clinical records and language learner essays. <a href='http://noisy-text.github.io/'>[Link]</a>"
29,"[W14] NewSum",11/04/19,8:30 AM,6:00 PM,TBD,Workshops,"The Workshop on New Frontiers in Summarization seeks to bring together researchers from a diverse range of fields (e.g., summarization, visualization, language generation, cognitive and psycholinguistics) for discussion on key issues related to automatic summarization. <a href='https://summarization2019.github.io/'>[Link]</a>"
30,"[W15] TextGraphs-19",11/04/19,8:30 AM,6:00 PM,TBD,Workshops,"Graph-based Methods for Natural Language Processing is a workshop series that publishes and promotes the synergy between the field of Graph Theory and Natural Language Processing. <a href='https://sites.google.com/view/textgraphs2019'>[Link]</a>"
31,"[W16] WAT 2019",11/04/19,8:30 AM,6:00 PM,TBD,Workshops,"The 6th Workshop on Asian Translation is a new open evaluation campaign focusing on Asian languages. <a href='http://lotus.kuee.kyoto-u.ac.jp/WAT/'>[Link]</a>"
32,"[W17] ECONLP 2019",11/04/19,8:30 AM,6:00 PM,TBD,Workshops,"The 2nd Workshop on Economics and Natural Language Processing addresses the increasing relevance of natural language processing (NLP) for regional, national and international economy, both in terms of already operational language technology products and systems, as well as newly emerging methodologies and techniques reflecting the requirements at the intersection of economics and NLP. <a href='https://sites.google.com/view/econlp-2019'>[Link]</a>"
33,"[W18] NLP4IF",11/04/19,8:30 AM,6:00 PM,TBD,Workshops,"NLP for Freedom: Censorship, Disinformation, and Propaganda is a workshop dedicated to NLP methods that potentially contribute (either positively or negatively) to the free flow of information on the Internet, or to our understanding of the issues that arise in this area. <a href='http://www.netcopia.net/nlp4if/'>[Link]</a>"
